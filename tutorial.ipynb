{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd121ab-8f62-45c5-bc2c-8a7854246074",
   "metadata": {},
   "source": [
    "# MLops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d41927-a360-48d6-b4cd-172049f058be",
   "metadata": {},
   "source": [
    "MLops est un ensemble de pratiques qui permettent d'automatiser les tâches de Machine Learning et l'optimisation de développement et déploiement des modèles. Dans ce tutoriel, on utilisera ``mlflow`` et ``argo workflows`` pour cela."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da2059-6955-407c-81e0-5fe67cf71c64",
   "metadata": {},
   "source": [
    "## 1- Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc892600-8f20-4d6e-9461-6b86a8489930",
   "metadata": {},
   "source": [
    "``MLflow`` est une plateforme qui permet de gérer des modèles de Machine Learning en enregistrant les paramètres et les métriques de performance. Il propose une interface utilisateur qui permet d'administrer et comparer les modèles facilement. Mlflow permet aussi de déployer les modèles via une API REST de manière simple et rapide. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c5d55-0038-44c4-af6e-87443172c989",
   "metadata": {},
   "source": [
    "On commence par lancer mlflow sur Datalab. ***Il faut que la protection IP soit désactivée.***\n",
    "<br> <img src=\"notebook-images/mlflow.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6529e-3c05-45eb-8e42-2c50198a7263",
   "metadata": {},
   "source": [
    "Ici, on va entrainer un modèle pour détecter les pizzas avec ``PyTorch``. Pour enregistrer les données avec mlflow, on suit les étapes suivantes:\n",
    "- Au début, on indique à mlflow le nom de notre expérience. mlflow sauvegardera donc les données dans un dossier ayant le même nom. Pour cela, on utilise la commande ``mlflow.set_experiment(mlflow_experiment_name:str)``\n",
    "- Avant de commencer la boucle for, on précise le contexte en utilisant la syntaxe ``with mlflow.start_run(run_name:str)``. Cela permet de suivre la progression de l'entrainement sur l'interface utilisateur mlflow ainsi que la création d'une instance sous le nom ``run_name`` où les données seront enregistrées. \n",
    "- Enregistrement des paramètres avec ``mlflow.log_parameter(param_name:str, param_value)``\n",
    "- Enregistrement des métriques avec ``mlflow.log_metric(metric_name:str, metric_value)``\n",
    "- Enregistrement du modèle avec ``mlflow.pytorch.log_model(model_name:str, model)``. mlflow est compatible avec plusieurs frameworks de Machine et Deep learning tels que PyTorch, Scikit-Learn, et TensorFlow.\n",
    "\n",
    "Le code complet de la fonction main est disponible dans le fichier ``main.py`` (lequel on va utiliser par la suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c86fe-534b-437b-ba4a-fcf97b206fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, weight_decay, epochs, batch_size, mlflow_experiment_name, mlflow_run_name):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device(\"cpu\")\n",
    "    batch_size = batch_size\n",
    "    data_root = \"diffusion/pizza-not-pizza\"\n",
    "\n",
    "    dataset = DatasetGenerator(data_root)\n",
    "    train_set_length = int(0.7 * len(dataset))\n",
    "    test_set_length = len(dataset) - train_set_length\n",
    "    #split the dataset \n",
    "    train_set, test_set = random_split(dataset, [train_set_length, test_set_length])\n",
    "    train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_set, shuffle=True, batch_size=batch_size)\n",
    "    class_num = 2\n",
    "    ce_loss=nn.CrossEntropyLoss()\n",
    "\n",
    "    #load model\n",
    "    model = ResNet(class_num=class_num)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #define optimizer\n",
    "    opt_model = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    epoch = epochs\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    #set experiment name\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "    #set context\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        for epo in range(1,epoch+1):\n",
    "            correct_model = 0\n",
    "            print(\"Epoch {}/{} \\n\".format(epo, epoch))\n",
    "            with tqdm(total=len(train_loader), desc=\"Train\") as pb:\n",
    "                for batch_num, (img, img_label) in enumerate(train_loader):\n",
    "                    opt_model.zero_grad()\n",
    "                    img = img.to(device) \n",
    "                    img_label = img_label.to(device)       \n",
    "                    outputs = model(img)\n",
    "                    correct_model += (torch.argmax(outputs, dim=1)==img_label).sum().item()\n",
    "                    loss = ce_loss(outputs, img_label)\n",
    "                    loss.backward()\n",
    "                    #loss_model.append(loss)\n",
    "                    opt_model.step()\n",
    "                    pb.update(1)\n",
    "        train_accuracy = correct_model/len(train_set)\n",
    "        val_accuracy = evaluate(model, test_loader, device)\n",
    "        mlflow.log_param(\"learning rate\", lr)\n",
    "        mlflow.log_param(\"weight decay\", weight_decay)\n",
    "        mlflow.log_metric(\"valdiation accuracy\", val_accuracy)\n",
    "        mlflow.log_metric(\"training accuracy\", train_accuracy)\n",
    "        #mlflow.pytorch.log_model(\"model\", model, code_paths=[\"model.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d1432-08f0-4c87-9116-52f81d3d345a",
   "metadata": {},
   "source": [
    "Avant de lancer l'entrainement, on doit définir les hyperparamètres et les variables d'environnement nécessaires pour mlflow. Pour ce faire, on utilisera le script shell suivant:\n",
    "```shell\n",
    "#set environment variables\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-mbenxsalha-561205.user.lab.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"Default\"\n",
    "\n",
    "\n",
    "# set hyper parameters\n",
    "lr=\"0.01\"\n",
    "weight_decay=\"0.001\"\n",
    "epochs=\"1\"\n",
    "batch_size=\"64\"\n",
    "mlflow_experiment_name=\"Default\"\n",
    "mlflow_run_name=\"test\"\n",
    "\n",
    "root_path=\"/home/onyxia/work/\"\n",
    "\n",
    "python ${root_path}/main.py --lr lr --weight_decay weight_decay --epochs epochs --batch_size batch_size --mlflow_experiment_name mlflow_experiment_name --mlflow_run_name mlflow_run_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b9226-7e8b-43e9-ad24-e91f81cd9f24",
   "metadata": {},
   "source": [
    "***Il faut changer la variable ``MLFLOW_TRACKING_URI`` au lien de votre service mlflow***.\n",
    "\n",
    "Ce script est enregistré dans le fichier ``local_mlflow.sh``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ce565-cfe9-465d-a8fb-3d7a4a9ed616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start training\n",
    "!sh local_mlflow.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ca633-1bfd-4663-ae87-04c0780d0346",
   "metadata": {},
   "source": [
    "On peut aussi lancer l'entrainement dans un environnement virtuel. Pour cela, il faut mettre en place deux fichiers de configuration:\n",
    "- ``conda.yaml``: dans ce fichier, on définit un environnement virtuel avec toutes les dépendances.\n",
    "- ``MLProject``: dans ce fichier, on définit les paramètres et la commande pour lancer l'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e039e-c5a0-4c53-8a97-3f6b52229773",
   "metadata": {},
   "source": [
    "``conda.yaml``: \n",
    "```yaml\n",
    "name: pizza-detector\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "    - torch\n",
    "    - torchvision\n",
    "    - boto3==1.17.19\n",
    "    - tqdm\n",
    "    - argparse\n",
    "    - mlflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58af7ca-3240-4d4d-bcfe-834b8c34e82c",
   "metadata": {},
   "source": [
    "``MLProject``:\n",
    "````\n",
    "name: pizza-detector\n",
    "\n",
    "conda_env: conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      remote_server_uri: {type: str, default: https://user-mbenxsalha-207868.user.lab.sspcloud.fr/}\n",
    "      experiment_name: {type: str, default: Default}\n",
    "      run_name: {type: str, default: default}\n",
    "      lr: {type: float, default: 0.01}\n",
    "      weight_decay: {type: float, default: 0.001}\n",
    "      epochs: {type: int, default: 1}\n",
    "      batch_size: {type: int, default: 64}\n",
    "      \n",
    "    command: \"python main.py --lr {lr} --weight_decay {weight_decay} --epochs {epochs} --batch_size {batch_size} --mlflow_experiment_name {experiment_name} --mlflow_run_name {run_name}\"\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54ecab-bc41-47db-9736-f3119af16a1e",
   "metadata": {},
   "source": [
    "```shell\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-mbenxsalha-561205.user.lab.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"Default\"\n",
    "\n",
    "run_name=\"default\"\n",
    "\n",
    "# set the hyper parameters\n",
    "lr=\"0.01\"\n",
    "weight_decay=\"0.001\"\n",
    "epochs=\"1\"\n",
    "batch_size=\"64\"\n",
    "\n",
    "mlflow run . -P remote_server_uri=${MLFLOW_TRACKING_URI} \\\n",
    "-P experiment_name=${MLFLOW_EXPERIMENT_NAME} -P run_name=${run_name}\n",
    "-P lr=${lr} -P weight_decay=${weight_decay} -P epochs=${epochs} -P batch_size=${batch_size}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f562a89-4e35-4e32-8596-f1150fee7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh remote_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580917b4-52ab-474f-b591-9c004312d9e5",
   "metadata": {},
   "source": [
    "## 2- Argo Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a3d9a-e16a-4562-8b17-e9cd00489188",
   "metadata": {},
   "source": [
    "Argo workflow est un workflow engine qui permet d'orchestrer des tâches de machine learning sur un cluster Kubernetes. Le workflow est défini par un graphe direct acyclique (DAG) où les noeuds définissent des tâches qui sont exécutée dans des conteneurs séparés parallèlement ou séquentiellement. Cela permet d'optimiser l'utilisation des ressources disponibles tout en accélérant le workflow remarquablement. \n",
    "\n",
    "Dans ce tutoriel, on va utiliser cet outil pour optimiser les hyperparamètres de notre modèle de classification de pizza. En l'occurence, on va entrainer plusieurs modèles avec des hyperparamètres différents et on va utiliser mlflow pour enregistrer les métriques de performance et comparer les différents modèles obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d59f27-82f0-4a83-91a8-09270426807c",
   "metadata": {},
   "source": [
    "Pour commencer, on doit lancer un nouveau service ``argo-workflows`` sur Datalab.\n",
    "<br> <img src=\"notebook-images/argo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639e0dc-0174-47c9-8380-fbf989facb68",
   "metadata": {},
   "source": [
    "On installe le client argo workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6844d3c-1088-4a8a-ad48-4e2b96ed24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo sh scripts/argo_deb_install.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c29bbf-abad-4201-9935-d4ddc92c2878",
   "metadata": {},
   "source": [
    "Ensuite, on configure un fichier ``workflow.yaml`` dans lequel on définit notre workflow. Ce workflow se décompose en trois parties:\n",
    "- configuration des paramètres\n",
    "- définition de DAG\n",
    "- implémentation de DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29588474-a868-4fd7-a665-d99512303d06",
   "metadata": {},
   "source": [
    "***1- Configuration des paramètres***\n",
    "\n",
    "On configure les paramètres de s3 pour pouvoir utiliser et enregistrer les données avec le serveur minio de SSPCloud. On configure aussi les paramètres de mlflow pour pouvoir communiquer avec le service mlflow. ``code-source-repo`` indique la repositoire github qui contient le code source de notre application. Ce repo doit contenir le fichier ``MLProject`` dans le chemin root. Le paramètre ``model-training-conf-list`` est une liste de dictionnaires contenant les différentes combinaisons d'hyperparamètres qu'on va tester.\n",
    "```yaml\n",
    "  arguments:\n",
    "    parameters:\n",
    "      - name: aws-access-id\n",
    "        value: \"changeme\"\n",
    "      - name: aws-secret-key\n",
    "        value: \"changeme\"\n",
    "      - name: aws-session-token\n",
    "        value: \"changeme\"\n",
    "      - name: aws-default-region\n",
    "        value: \"us-east-1\"\n",
    "      - name: aws-s3-endpoint\n",
    "        value: \"minio.lab.sspcloud.fr\"\n",
    "      - name: mlflow-tracking-uri\n",
    "        value: 'https://user-mbenxsalha-561205.user.lab.sspcloud.fr/'\n",
    "      - name: mlflow-experiment-name\n",
    "        value: \"Default\"\n",
    "      - name: mlflow-s3-url\n",
    "        value: \"https://minio.lab.sspcloud.fr\"\n",
    "      - name: code-source-repo\n",
    "        value: \"https://github.com/amine-bs/mlflow-argo.git\"\n",
    "      - name: model-training-conf-list\n",
    "        value: |\n",
    "          [\n",
    "            { \"lr\": \"0.01\", \"weight_decay\": \"0.001\", \"mlflow_run_name\": \"1\"},\n",
    "            { \"lr\": \"0.01\", \"weight_decay\": \"0.0001\", \"mlflow_run_name\": \"2\"},\n",
    "            { \"lr\": \"0.05\", \"weight_decay\": \"0.0001\", \"mlflow_run_name\": \"3\"},\n",
    "            { \"lr\": \"0.05\", \"weight_decay\": \"0.001\", \"mlflow_run_name\": \"4\"}\n",
    "\n",
    "          ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf7780a-e464-4584-b8cc-75fd256bbd02",
   "metadata": {},
   "source": [
    "***2- Définition de DAG***\n",
    "\n",
    "Ici, on définit les dépendances et les paramètres de chaque étape.\n",
    "```yaml\n",
    "    - name: main\n",
    "      dag:\n",
    "        tasks:\n",
    "          # task 0: start pipeline\n",
    "          - name: start-pipeline\n",
    "            template: start-pipeline-wt\n",
    "          # task 1: train model with given params\n",
    "          - name: train-model-with-given-params\n",
    "            dependencies: [ start-pipeline ]\n",
    "            template: run-model-training-wt\n",
    "            arguments:\n",
    "              parameters:\n",
    "                - name: lr\n",
    "                  value: \"{{item.lr}}\"\n",
    "                - name: weight_decay\n",
    "                  value: \"{{item.weight_decay}}\"\n",
    "                - name: mlflow_run_name\n",
    "                  value: \"{{item.mlflow_run_name}}\"\n",
    "\n",
    "              # pass the inputs to the step \"withParam\"\n",
    "            withParam: \"{{workflow.parameters.model-training-conf-list}}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccd199-c815-4d71-a871-b291ffea3a2b",
   "metadata": {},
   "source": [
    "***3- Implémentation de DAG***\n",
    "\n",
    "Ici, on définit les images ,les commandes et les variables d'environnement à utiliser pour chaque conteneur de chaque étape. Pour la première étape, on utilise une image de ``busybox`` pour initialiser le pipeline. Pour la deuxième étape, on utilise une simple image contenant ``conda``.\n",
    "```yaml\n",
    "    - name: start-pipeline-wt\n",
    "      inputs:\n",
    "      container:\n",
    "        image: busybox\n",
    "        command: [ sh, -c ]\n",
    "        args: [ \"echo start pipeline\" ]\n",
    "\n",
    "    # worker template for task-1 train model\n",
    "    - name: run-model-training-wt\n",
    "      inputs:\n",
    "        parameters:\n",
    "          - name: lr\n",
    "          - name: weight_decay\n",
    "          - name: mlflow_run_name\n",
    "      container:\n",
    "        image: liupengfei99/mlflow:latest\n",
    "        command: [sh, -c]\n",
    "        args: [\"mlflow run $CODE_SOURCE_URI --version main -P remote_server_uri=$MLFLOW_TRACKING_URI -P experiment_name=$MLFLOW_EXPERIMENT_NAME -P lr={{inputs.parameters.lr}} -P weight_decay={{inputs.parameters.weight_decay}} -P mlflow_run_name={{inputs.parameters.mlflow_run_name}}\"]\n",
    "        env:\n",
    "          - name: AWS_SECRET_ACCESS_KEY\n",
    "            value: \"{{workflow.parameters.aws-secret-key}}\"\n",
    "          - name: AWS_DEFAULT_REGION\n",
    "            value: \"{{workflow.parameters.aws-default-region}}\"\n",
    "          - name: AWS_S3_ENDPOINT\n",
    "            value: \"{{workflow.parameters.aws-s3-endpoint}}\"\n",
    "          - name: AWS_SESSION_TOKEN\n",
    "            value: \"{{workflow.parameters.aws-session-token}}\"\n",
    "          - name: AWS_ACCESS_KEY_ID\n",
    "            value: \"{{workflow.parameters.aws-access-id}}\"\n",
    "          - name: MLFLOW_TRACKING_URI\n",
    "            value: \"{{workflow.parameters.mlflow-tracking-uri}}\"\n",
    "          - name: MLFLOW_EXPERIMENT_NAME\n",
    "            value: \"{{workflow.parameters.mlflow-experiment-name}}\"\n",
    "          - name: MLFLOW_S3_ENDPOINT_URL\n",
    "            value: \"{{workflow.parameters.mlflow-s3-url}}\"\n",
    "          - name: CODE_SOURCE_URI\n",
    "            value: \"{{workflow.parameters.code-source-repo}}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f7b19-1928-460f-8afb-94dabcedf191",
   "metadata": {},
   "source": [
    "Enfin, on soumet notre workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6433e3-6dbe-4c23-938d-50ae83ff3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "!argo submit workflow.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73078e0b-aa62-4291-af66-01d9132c9b45",
   "metadata": {},
   "source": [
    "On peut voir les conteneurs qui viennent de se lancer avec ``kubectl``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71db090-4ff6-4070-8a58-ca60d90f47ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b02ed-5fea-420a-a34b-3dc4865a3fd0",
   "metadata": {},
   "source": [
    "On peut aussi suivre la progression et les logs de chaque conteneur sur l'interface utilisateur d'Argo.\n",
    "<img src=\"notebook-images/argo-ui.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfed0a9-673b-4312-9425-041e2405c112",
   "metadata": {},
   "source": [
    "On peut comparer les résultats obtenus de différents hyperparamètres en utilisant mlflow.\n",
    "   <tr>\n",
    "    <td> <img src=\"notebook-images/mlflow-ui.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "    <td> <img src=\"notebook-images/mlflow-compare.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/> </td>\n",
    "    </tr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
